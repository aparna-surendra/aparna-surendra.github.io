---
layout: default
---

<html>
<style>
#content {
  float: left;
}
text {
  font: 11px "Proxima Nova",sans-serif;
  text-anchor: left;
}

</style>
<head>
	<script src="https://d3js.org/d3.v4.min.js"></script>
	<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>
</head>
<body>
 <header>
    <h2><a href="page1.html">PREV</a>  ||  <a href="new_page3.html">NEXT</a></h2>
    <h1>A reinforcement learning primer</h1>
    <h2>2/5 From Intuition to RL Fundamentals</h2>
  </header>
<div id="content">
<div id="" style="overflow-y:scroll; ">
<header>
<h3>1. Intuition </h3>
</header>
<p><b><i>How does an RL algorithm learn from first principles, through trial-and-error?</i></b></p>
<p> Let’s consider a simple 4 x 4 grid environment. In many respects, the algorithm is doing what a human would do when faced with the following instructions (and no other detail):</p>
<p>__Interact with the following environment and collect as high a total reward as possible__</p>
</div>
<div align="center">
<div id="grid"></div>
<script src="grids/grid.js" type="text/javascript"></script>
</div>

<p>After training with the environment, you should be able to hit ‘reset’ and play the game for +4 (the maximum possible total rewards), using the optimal strategy – playing along the diagonal (detailed below).</p>

<div align="center">
<div id="grid0"></div>
<script src="grids/grid27.js" type="text/javascript"></script>
</div>
<p>
In many ways, reinforcement learning can be considered the computational approach to what you just did: you approached a new environment with a given goal, interacted with the environment to learn by trial-and-error, and identified the best possible strategy to achieve the goal. </p> 
</p>
<p> Through interaction, you would have identifed: 
<ul> <li> <b>Legal actions:</b> The game must begin on the top-left square; You can only move along adjacent squares within the grid. </li></ul>
<ul> <li> <b>Reward details:</b> The square along the diagonal have a positive reward (+1, coloured in blue). All other squares have a negative reward (-1, coloured in orange). You can only receive one reward per square per game. You can return to a square within a game, but will not receive a reward for this action. </li> </ul>
</p>

<header>
<h3> 2. Introducing RL Concepts</h3>
</header>
<p> Let's introduce some reinforcement learning terminology: 
  <ul> <li> Our 4 x 4 grid is an <b>'environment'</b>. A representation of an environment is known as a <b>'state'</b>. For simplicity, we can refer to each square in our grid as a state (this is not quite correct, but serves the purpose of this primer). The concept of a state will soon become clearer, especially when we discuss environments that better-resemble the real world. </li></ul>
  <ul> <li> The move from one square (state) to another is known as an <b>'action'</b>. </ul></li></p>
  <ul> <li> The points received for each state is known as a <b>'reward'</b> </ul></li>
  <ul> <li> The sequence to follow (the mapping of states to actions) is known as the <b>'policy'</b> </ul></li>
</p>
<p>
To make it more concrete, let's frame our optimal strategy using these new terms:</p>
<p> Our first <b>action</b> is to move (from the off-the-grid <b>state</b>) to State 0 <b>state</b>, receiving a <b>reward</b> of +1. We then follow a <b> policy</b> from State 0 to 5,froom State 5 to 10, and State 10 to 15. In reinforcement learning, we will train an <b>agent</b> to do the same. </p>
<p>

</div>
</body>
<div align="center">
<h2><a href="page1.html">PREV</a>  ||  <a href="new_page3.html">NEXT</a></h2>
</div>
</html>
