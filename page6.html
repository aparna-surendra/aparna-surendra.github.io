---
layout: default
---

<html>
<style>
#content {
  float: left;
  width: 800px;
  margin: 0;
}
text {
  font: 11px "Proxima Nova",sans-serif;
  text-anchor: left;
}
.column {
  float: left;
  width: 45%;
  padding: 10px;
}

.row::after {
  content: "";
  clear: both;
  display: table;
}

ul, ol {
    margin: 0;
}

p, ul {

    margin: 15px;

}

  th {
            text-align: right;
            font-weight: bold;
        }

        table {
            text-align: right;
            padding-right: 2px;
            margin-right: 3px;
            width: 100%;
        }

        td:nth-child(6) {
            margin-right: 8px;
        }

        td:nth-child(1) {
            width: 200px;
        }

</style>
<head>
	<script src="https://d3js.org/d3.v4.min.js"></script>
	<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>
</head>
<body>
 <header>
    <h1>Introduction to reinforcement learning</h1>
    <h3>1/5 Introducing an RL algorithm <a href="page5.html"><img src="images/right_arrow.png"style="width:10px;height:15px;"></a></h3>
  </header>
<div id="content">
<div id="" style="overflow-y:scroll; height:600px;">
<p>
We now understand the intuition behnd reinforcement learning: an agent maximises reward over the long-term, by selecting actions that lead to 'high value' states. But how does an agent 'learn' a value function? 

We can return to the 4 x 4 grid. We have talked about calculating values for each state (by calculating long-term rewards), but we can add more rigor to this. 

</p>
<div align="center">
<div id="grid"></div>
<script src="grids/grid8.js" type="text/javascript"></script>
</div> 

<p> First, let's move to  appropriate terminology. Every move can be considered a time-step  \(t\), and all states, actions, and rewards are represented by the time step in which they are experienced (states  \(s\)), taken (actions \(a\)), or received (rewards  \(r\)). </p>

<p> Let's consider the following sequence: I begin the game by moving from Square 0 to Square 1 and receive reward -1. I then move from Square 1 to Square 5, and receive reward +1. </p>
<ul> \(s_t\ =\) square 0 ,   \(a_t\ = \) left ,   \(r_t = \) -1 ,  \(s_{t+1} = \) square 1 </ul>
<ul> \(s_{t+1}\ =\) square 1 , \(a_{t+1}\ = \) down, \(r_{t+1} = \) +1 , \(s_{t+2} = \) square 5 </ul>

<p>Now, let's estimate the value functions. We do this by: <ul> 1) Evaluating all the legal actions available to us </ul> <ul> 2) Estimating the cumulative rewards for the optimal policy beginning with state \(s_{t+1} \) </ul></p>

<p>
Consider the first move, from off-the-grid to Square 0. </p>
<ul><li> There is only one legal action – moving to \(s_{t+1}\) Square 0.</ul></li>
<ul><li> Estimate the maximum value of Square 0 over all available legal actions </ul></li>
<ol>
<ul><li>  Consider all legal actions to \(s_{t+2}\), which can be Square 0, 1, 2, 6, 10, 9, 8, or 4. </ul></li>
<ul><li>  For each \(s_{t+2}\), estimate the maximum value over all available legal actions.  </ul></li>
<ul><li> And so on ... </ul></li>
</ol>
<p>This is programmatically defining what was already intuitive to us. Let's make this more concrete and calculate some values using this method. If we consider  first states as off-the grid,  </p>
<div id="grid3"></div>
<table id="table"> </table>
<script src="grids/grid13.js" type="text/javascript"></script>

</div>
</div>
</body>
</html>
