---
layout: default
---

<html>
<style>
#content {
  float: left;
}
text {
  font: 11px "Proxima Nova",sans-serif;
  text-anchor: left;
}
.column {
  float: left;
  width: 45%;
  padding: 10px;
}

.row::after {
  content: "";
  clear: both;
  display: table;
}

.column2 {
  float: left;
  width: 32%;
}

ul, ol {
    margin: 0;
}

p, ul {

    margin: 15px;

}

 th {
            text-align: left;
            font-weight: bold;
      }

 table {
         text-align: left;
         padding-right: 2px;
         margin-right: 3px;
         width: 100%;
        }

td, th {
border-left: solid 1px black;
} 

</style>
<head>
	<script src="https://d3js.org/d3.v4.min.js"></script>
	  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
	<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>
</head>
<body>
 <header>
    <h1>A reinforcement learning primer</h1>
    <h2><a href="new_page5.html"><img src="images/left_arrow.png"style="width:8px;height:13px;"></a>OPTIONAL: Introducing an RL algorithm</h2>
  </header>
<div id="content">
<b>How does an agent 'learn' values? </b> <p></p>
<p></p>
<i>An introduction to the Q-learning algorithm</i> <p></p>
<p>Let's return to a 4 x 4 grid with $+1$ rewards along the diagonal.</p>
<p>This tutorial will walk through how Q-values are updated through each training episode (with manual calculations). It makes several simplifying assumptions, but these serve the purpose of a soft introduction. </p>

<h3>Q-learning algorithm</h3>
<h4>1. Initialize Q-values</h4>
<p> If we were to approach the grid with no prior knowledge, we consider each state the same – no one state is better or worse than the other. We initialize all $Q(s,a)$ values to $0$, as displayed below.</p>
<div class="row">
<div class = "column">
<div id="grid"></div>
<script src="grids/grid8.js" type="text/javascript"></script>
</div> 
<div class="column">
<div id="grid3"></div>
<script src="grids/grid15.js" type="text/javascript"></script>
</div>
</div>
<header>
<h4>2. Begin Training – Training Episode 1</h4>
</header>
<p> In Training episode 1, we select actions randomly (as all states are equally desirable/have the same Q-value), and terminate the episode after 7 steps (this is to simplify the problem, there are no game constraints that require this).</p>

<div class="row">
<div class="column">

<b>Time Step 1 </b><p></p> moves off-the-grid to Square 0. We receive a reward ($r_t$) of +1 and can update our Q-value for moving to State 0. 
$$Q(s,a) = r_{t} + max[(Q(s_{t+1},a_{t+1})]$$
$$Q(\text{s = off-grid}, \text{a = right}) = 1 + 0$$
$$                                          = 1$$ 
To simplify our grid, we label State 0 with the updated Q-value. This is the Q-value for moving <i>to</i> State 0 </p>. (Note - Q-values are only updated upon moving to the new state).

</div>
<div class="column" style = "padding-left: 50px">
<div id="grid4"></div>
<script src="grids/grid16.js" type="text/javascript"></script>
</div> 
</div>

<div class = "row">
<div class = "column" style = " padding-left: 20px" >
<div id="grid6"></div>
<script src="grids/grid18.js" type="text/javascript"></script>
</div>
<div class="column" style = "padding-top: 20px">
<b>Time Step 2</b> <p></p> From State 0, all actions ${a_t}$ (right, down, right-diagonal) seem to lead to equivalent $s_{t+1}$ (States 1, 4, and 5). <p></p>
We randomly elect to move to State 1 and receive a reward ($r_t$) of -1. We can update the Q-value for moving to State 1. 
$$Q(s,a) = r_{t} + max[(Q(s_{t+1},a_{t+1})]$$
$$Q(\text{s = 0}, \text{a = right}) = -1 + 0$$
$$                                  = -1$$ 
</div>
</div>


<p><b>And so on, </b> until the end of the training episode. This produces the updated Q-values below.</p><p></p>

<div align="center">
<div id="grid5"></div>
<script src="grids/grid17.js" type="text/javascript"></script>
</div>

<header>
<h4>3. Training Episode 2</h4>
</header>
<b>Time Step 1</b> <p></p> 
By following a 'greedy' policy (selecting the highest value state-actions without exploring further), we move to Square 0 and receive +1. <p></p>
Let's update the Q-value of moving to State 1:
<ul><li> $r_{t}$ is the same, it equals $1$. </li></ul>
<ul><li> But $max. Q(s_{t+1}, a_{t+1})$ values have changed – moving right to State 1 has a Q-value of $-1$, moving diagonally to State 5 has one of $+1$, while moving down to State 4 has the same Q-value of $0$. </li></ul>
 <p></p>
$$Q(s,a) = r_{t} + max[(Q(s_{t+1},a_{t+1})]$$
$$Q(\text{s = off-grid}, \text{a = right}) = 1 + max[1,-1,0]$$
$$                                         = 1 + 1 = 2$$ 

We update the Q-value of State 0 (shorthand for the Q-value of moving <i>to</i> State 0) to $2$. <p></p>
<b>Time Steps 2,3,4</b><p></p>
We follow the greedy policy (move from State 5 - State 10 - State 15) and update the Q-values accordingly.<p></p> 
 
<p>For State 5, update the $max. Q(s_{t+1}, a_{t+1})$ value to 1 : $Q(s_t,a_t) = r_{t} + max[(Q(s_{t+1},a_{t+1})] = 1+1 = 2$ </p>
<p>For State 10, update the $max. Q(s_{t+1}, a_{t+1})$ value to 1 : $Q(s_t,a_t) = r_{t} + max[(Q(s_{t+1},a_{t+1})] = 1+1 = 2$ </p>
<p>For State 15, the update does not change the previous Q value : $Q(s_t,a_t) = r_{t} + max[(Q(s_{t+1},a_{t+1})] = 1+0 = 1$ </p>

<header>
<h4>4. Training Episode 3</h4>
</header>
Follow the greedy policy, and move from State 0 - State 5 - State 10 - State 15. 
<p>For Square 0, update the $max. Q(s_{t+1}, a_{t+1})$ value to 1: $Q(s_t,a_t) = r_{t} + max[(Q(s_{t+1},a_{t+1})] = 1+2 = 3$</p>
<p>For Square 5, update the $max. Q(s_{t+1}, a_{t+1})$ value to 2 : $Q(s_t,a_t) = r_{t} + max[(Q(s_{t+1},a_{t+1})] = 1+1 = 3$ </p>
<p>For Squares 10 and 15, the update does not change the previous Q-values</p>

<header>
<h4>5. Training Episode 4 and convergence </h4>
</header>
Follow the greedy policy again, and move from State 0 - State 5 - State 10 - State 15. This time there is only one update: the Q-value of moving to Square 0 is updated to 4. 
<p></p> In subsequent training episodes, the Q-values stay the same.This is known as <b>'convergence' </b>. The agent has learned the Q-values, and 'learning' is complete . <p></p>


<div class = "row">
<div class = "column2" >
<div id="grid7"></div> 
<script src="grids/grid19.js" type="text/javascript"></script>
</div>
<div class="column2">
<div id="grid8"></div>
<script src="grids/grid20.js" type="text/javascript"></script>
</div>
<div class="column2">
<div id="grid9"></div>
<script src="grids/grid21.js" type="text/javascript"></script>
</div>
</div>

<header>
<h4>A comment on exploration/exploitation </h4>
</header>
This illustration makes several simplifying assumptions,  including: we find the optimal policy in the first training episode; we always follow a greedy policy. If we were to explore during learning, there would be higher Q-values for the states adjacent to the diagonal (as they are 'better' than those further away). <p></p>
As discussed in <a href = ="new_page4.html">Section 4 - Exploration/Exploitation</a>, exploration would increase the likelihood of identifying the optimal policy, but increase training time (to frame it with a new concept, it would take longer for the Q-values to <b>converge</b>). 

<h3>Introducing Learning Rate</h3>
The illustration above has several simplifying assumptions that allow us to 'update' Q-values by replacing them with the most recent calculation. But what happens if rewards are not static – e.g. if State 15 alternates between rewards of  +10 and -100?  <p></p>
In the method we've outlined, our Q-value for Square 15 would swing to capture the most recent reward, and wouldn't indicate the average behaviour. As a consequence, the Q-values wouldn't converge.<p></p>

We address this by introducing a learning rate $\alpha$, which controls the speed of learning. <p></p>

We can update the method we were using before: 
$$Q(s,a) = Q(s,a)_{new}$$
Which can be represented as $$Q(s,a) =  Q(s,a)_{old} + [Q(s,a)_{new} - Q(s,a)_{old}] $$

And now include the learning rate $\alpha$.
$$Q(s,a) =  Q(s,a)_{old} + \alpha[Q(s,a)_{new} - Q(s,a)_{old}]$$

When $\alpha = 1$, we are learning by completely replacing the old Q-value with the new Q-value (as in the previous examples). If $\alpha = 0$, we are not learning at all. <p></p>

To make this more concrete, let us consider the example where Square 15 has a 50% probability of a +10 reward, and 50% probability of -100. <p></p>
<b>Option 1: </b> $\alpha = 1$, which is equivalent to: $Q(s,a) = Q(s,a)_{new}$ <p></p>
Here, we learn in the same fashion as the previous example. In the first training episode, Square 15 reveals a reward of +10. In the second, -100. In the third, +10. Look at the Q-values as they update over the three episodes. Unlike in the previous example, the values will not converge in the 4th training episode – they will continue to swing back-and-forth. <p></p>
<div class = "row">
<div class = "column2">
<div id="grid10"></div>
<script src="grids/grid22.js" type="text/javascript"></script>
</div> 
<div class = "column2">
<div id="grid11"></div>
<script src="grids/grid23.js" type="text/javascript"></script>
</div>
<div class = "column2">
<div id="grid12"></div>
<script src="grids/grid24.js" type="text/javascript"></script>
</div>  
</div>
<p></p>
<b>Option 2: $\alpha = 0.5$</b> <p></p>
When $\alpha = 0.5$, the update rule is: $$Q(s,a) =  Q(s,a)_{old} + 0.5[Q(s,a)_{new} - Q(s,a)_{old}]$$ 
Now, let's follow the same sequence. In the first step, we move from off-the-grid to Square 0. We update the Q-value of Square 0 as follows:<p></p>
<p>$Q(s,a)_{old} = 0$ </p> 
<p>$Q(s,a)_{new} = r_t + max[(Q(s_{t+1}), a_{t+1})] = 1 + 0 = 1$ </p> 
<p>$Q(s,a) =  Q(s,a)_{old} + 0.5[Q(s,a)_{new} - Q(s,a)_{old}] = 0 + 0.5[1-0] = 0.5$ </p>
And so on. As can be seen, the Q-values will not converge in the 4th training episode, but – eventually - they will tend towards the expected value for Square 15, which is -45 $[(0.50*-100) + (0.50*10)]$, representing that -on-average- it is not a desirable state. 
<div class = "row">
<div class = "column">
<div id="grid13"></div>
<script src="grids/grid25.js" type="text/javascript"></script>
</div> 
<div class = "column">
<div id="grid14"></div>
<script src="grids/grid26.js" type="text/javascript"></script>
</div> 
</div>  
<p><b>Introducing Exploration and Discounting</b> <p></p>
<p>As described previously, we can include exploration (using the $\epsilon$-greedy strategy - described in <a href = ="new_page4.html">Section 4</a>), and include discount factor in our value calculations (described in <a href = ="new_page3.html">Section 3</a>).</p>
</p>
</div>
</body>
</html>
