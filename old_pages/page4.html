---
---
layout: default
---

<html>
<style>
#content {
  float: left;
  width: 1100px;
  margin: 0;
}
text {
  font: 11px "Proxima Nova",sans-serif;
  text-anchor: left;
}
.column {
  float: left;
  width: 45%;
  padding: 10px;
}
.row::after {
  content: "";
  clear: both;
  display: table;
}

.text_label {
  font: 11px "Proxima Nova",sans-serif;

}

td {
border-left: solid 0.5px black;
text-align: center;
font: 9px "Proxima Nova",sans-serif;
} 

th {
text-align: center;
font: 9px "Proxima Nova", sans-serif;
font-weight: bold;
} 

td:nth-last-child(2) {text-align:left;}
td:nth-last-child(1) {text-align:left;}


</style>
<head>
  <script src="https://d3js.org/d3.v4.min.js"></script>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>
</head>
<body>
 <header>
    <h1>Introduction to reinforcement learning</h1>
    <h3>4/5 Initial intuition <a href="page5.html"><img src="images/right_arrow.png"style="width:10px;height:15px;"></a></h3>
  </header>
<div id="content">
<div id="" style="overflow-y:scroll; height:600px;">
<p>
The 4 x 4 grid environment is simplistic, but it expresses some key elements in a reinforcement learning system. The agent’s objective is to maximise the total reward it receives over the long run. Ultimately, our strategy involves a series of squares (known as <b>‘states’</b>) and <b>actions</b>. This mapping of states to actions that are taken when in those states can be considered a <b>policy</b>. The policy that yields the highest cumulative long-term reward is known as the <b>optimal policy</b>. 
</p>
<div align="center">
<div id="grid"></div>
<script src="grids/grid8.js" type="text/javascript"></script>
</div>
<p>At each time-step, the agent receives a <b>reward signal</b> (either positive or negative), which defines the good and bad events for the agent. But success in this game is not based on selecting a state for its immediate reward – it is based on selecting it for its cumulative, long-term reward (the rewards attached to a policy that begin with this state), which is known as the state’s <b>value</b>. We select states based on their estimated value: if State A has a higher value than State B, we will select State A. 
</p> 
<p>
To make this more concrete, consider a revised environment. The game must begin with a move to State 2, and completed within 4 moves. The first move has been made for you – you have 3 left. Experiment to find the optimal policy.
</p>
<div class = "row">
<div class="column">
<div id="grid2"></div>
<script src="grids/grid9.js" type="text/javascript"></script>
</div>
<div class="column">
<div id="grid1"></div>
<script src="grids/grid10.js" type="text/javascript"></script>
</div>
</div>
<p>From State 2, you can choose to move to State 3 or State 5. If we were to consider the immediate reward, the states would be equivalent – you receive +1 for moving to either state. However, if we consider the value – which is the long-term cumulative reward received from a policy that begins with a given state – State 5 is more desirable than State 3. Policy #1 shows the highest value policy for the game when moving from State 2 to State 3, which is a cumulative reward of +1.  Policy #2 shows the highest value policy for the game when moving from State 2 to State 5, which is a cumulative reward of +2. Policy #2 is the optimal policy for the game. 
 </p>
<div class = "row">
<div class="column">
<div id="grid4"></div>
<script src="grids/grid11.js" type="text/javascript"></script>
</div>
<div class="column">
<div id="grid5"></div>
<script src="grids/grid12.js" type="text/javascript"></script>
</div>
</div>
<p> 
We can also introduce the notion of <b>discounting</b>. Consider two policies: Policy 1 yields +100 over 1 time step, while Policy 2 yields +100 in cumulative reward over 100 time steps. Are both policies equivalent, or would you prefer Policy 2? 
</p>
<p>
We can express this mathematically through introducing a <b>discount factor ($\gamma$)</b> for each future time step – rewards received in the future are less valuable than rewards received immediately. If time step $t$ begins at 0, the reward is discounted at discount factor ($\gamma^t$). At $t_0$, the reward $r_0$ is discounted at $\gamma^0$; at $t_1$, the reward $r_1$ is discounted at $\gamma^1$ and so on. 
<ul><li>If the discount factor ($\gamma$) = 1, then future rewards are weighted the same as immediate rewards. Policy 1 \(=\) Policy 2  \(= 100\).</li></ul>
<ul><li>If $\gamma$ = 0.8, then Policy 1 has a cumulative discounted reward of $100$, while Policy 2 has one of $5$ (and is much less desirable). <i>Calculations:</i> Policy 1 $ = 100*0.8^0$; Policy 2 =  $1*0.8^0+ 1*0.8^1 + 1*0.8^2...1*0.80^{99}$ </li> </ul>
<ul><li>If $\gamma$ = 0.5, then Policy 1 has a cumulative discounted reward of $100$, while Policy 2 has one of $2$ (and is even less desirable). <i>Calculations:</i> Policy 1 $ = 100*0.5^0$; Policy 2 =  $1*0.5^0+ 1*0.5^1 + 1*0.5^2...1*0.50^{99}$</li> </ul>
</p>
<div class = "row">
<div class="column">
<div id="grid3"></div>
<p>
  <label for="nRadius" 
         style="display: inline-block; width: 180px; text-align: left">
         Discount Factor ($\gamma$)= <span id="nRadius-value">0.5</span>       
  </label>
  <input type="range" min="0" max="1" step="0.1" id="nRadius">
</p>
<div class = "text_label">
<p><b><i>Adjust discount factor and estimate values of policies</b></i></p>
</div>
</div>
<div class="column">
<table id="table"></table>
</div>
<script src="grids/grid14.js" type="text/javascript"></script>
</div>
</div>
</div>
</body>
</html>