---
layout: default
---

<html>
<style>
#content {
  float: left;
  width: 800px;
  margin: 0;
}
text {
  font: 11px "Proxima Nova",sans-serif;
  text-anchor: left;
}

.column {
  float: left;
  width: 45%;
  padding: 10px;
}

.row::after {
  content: "";
  clear: both;
  display: table;
}

</style>
<head>
	<script src="https://d3js.org/d3.v4.min.js"></script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>
 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
</head>
<body>
 <header>
    <h1>A reinforcement learning primer</h1>
    <h3><a href="new_page3.html"><img src="images/left_arrow.png"style="width:8px;height:13px;"></a> 4/5 Exploration/Exploitation <a href="new_page5.html"><img src="images/right_arrow.png"style="width:10px;height:15px;"></a></h3>
  </header>
<div id="content">
<div id="" style="overflow-y:scroll; height:600px;">
<header>
<h3> i. Intuition </h3>
</header>
<p>The <b>exploration-exploitation dilemma</b> is specific to reinforcement learning: should the agent <b>'explore'</b> the environment further, by selecting new state-action combinations or should it pursue (<b>'exploit'</b>) known policies that it found effective in the past?</p>
<p> We have not yet encountered this dilemma because we have been able to comprehensively explore our grid, often in a single training episode. To introduce some intuition, let's use a larger 20 x 20 grid game with one new rule: the game will end after 300 moves or 5 minutes. </p>
<p>All other constraints are the same: the game begins at the top-left square; movement must take place along neighboring squares on the grid; and a reward can only be received once per square per episode.
</li></ul> 
</p>
<p>The first move has been made for you. Interact with the environment, and try to find the optimal policy. </p>
<div align="center">
<div id="grid"></div>
<script src="grids/grid2.js" type="text/javascript"></script>
</div>
<p> This is not as easy as with a 4 x 4 grid: the 20 x 20 grid has 401 states, compared to the 17 states in the 4 x 4.</p>
<p>You play two games to 300 moves and reveal the following rewards.</p>
<div class = "row">
<div class="column">
<div id="grid2"></div>
<script src="grids/grid4.js" type="text/javascript"></script>
</div>
 <div class="column">
<div id="grid5"></div>
<script src="grids/grid5.js" type="text/javascript"></script>
</div>
</div>
<p>What policy will you pursue in the next game? </p>
<ul><li><i>Exploitation</i>: The <b>known optimal policy</b> that yields a $+180$ cumulative reward. <ul><li>Play all the blue $+1$ squares for a total of $+180$ , avoid the orange $-1$ squares, and wait for the clock to run out.</li></ul></li></ul>
<ul><li><i>Exploration</i>: A <b>new policy</b> that explores unseen state-action combinations, and will yield an unknown reward.</li></ul>
<p>As it turns out, you have not yet learned the optimal policy for the game...</p>
<div class = "row"> 
<div class="column">
<div id="grid6"></div>
<script src="grids/grid6.js" type="text/javascript"></script>
</div>
<div class="column">
<div id="grid7"></div>
<script src="grids/grid7.js" type="text/javascript"></script>
</div>
</div>
<p>If you pursued the known optimal policy (exploitation), you would not have learned the optimal policy for the game – which yields $+238$ (the policy* is visualised above) by selecting the cluster of $+1$ blue states on the right-hand side of the grid. But this is a trade-off! In instances where an agent has sufficient experience, additional exploration will delay learning. Balancing exploitation and exploration is key to successful reinforcement learning. </p>
<i>*Note: This is one of several optimal policies that lead to $+238$, as there is: 1) no penalty for re-selecting states and 2) no incentive to finish the game as quickly as possible. If we were to introduce one or both considerations, this would be the sole optimal policy. </i>
<header>
<h3> ii. Implementation </h3>
</header>
<p> We can implement exploration/exploitation programmatically using an $\epsilon$ - greedy strategy during the training period. With some probability $\epsilon$ the agent will select a random action, otherwise the agent will select a 'greedy' strategy (the action per the known optimal policy). <ul><li>When $\epsilon = 0.3$: There is a $30\%$ probability that the agent selects a random action, and a $70\%$ probability the agent selects the greedy action. </li></ul> In a popular implementation of $\epsilon$-greedy, $\epsilon$ decays with time – we assume that an agent with more experience requires less exploration. We can watch outcomes from $\epsilon$ - greedy strategy in the following WIRED/DeepMind video, which snapshots an agent's behaviour at various stages of training. </p>
 <p>With 600 training episodes (0.38s into the video), the agent learns a new optimal strategy to play the Atari game 'Breakout' (one that a casual human player, such as myself, did not know beforehand). Without exploration, the agent would not have necessarily learned this new optimal strategy – it would most likely have continued to exploit its previous strategy.</p>
<div align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/Q70ulPJW3Gk" frameborder="10" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
</div>
</body>
</html>
