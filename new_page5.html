---
layout: default
---

<html>
<style>
#content {
  float: left;
}
text {
  font: 11px "Proxima Nova",sans-serif;
  text-anchor: left;
}
.column {
  float: left;
  width: 45%;
  padding: 10px;
}

.row::after {
  content: "";
  clear: both;
  display: table;
}


</style>
<head>
	<script src="https://d3js.org/d3.v4.min.js"></script>
	<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
</head>
<body>
 <header>
    <h1>A reinforcement learning primer</h1>
    <h2><a href="new_page4.html"><img src="images/left_arrow.png"style="width:8px;height:13px;"></a> 5/5 From gridworld to the real world <a href="page7.html"><img src="images/right_arrow.png"style="width:10px;height:15px;"></a></h2>
  </header>
<div id="content">
<header>
<h3> 1. Introducing game environments </h3>
</header>
<p> In 2015, DeepMind <a href="https://deepmind.com/research/dqn/">published a breakthrough</a> in reinforcement learning research. The team had developed a new algorithm, the Deep-Q Learning Network (DQN), which could train on 46 different Atari games and had reached/exceeded human performance on 29 games. At its core, DQN is an algorithm that learns Q-values (and is a modified version of the Q-learning algorithm, which is included in the <a href="new_page1.html">supplementary tutorial</a>.</p>
<p>
DQN was a first demonstration of a generalisable or general-purpose agent – the same algorithm could be trained on a diverse set of environments without modification (this does not mean that a DQN agent trained to play Pong will perform well in a game of Breakout; the agent must train in the environment which it will play). DQN could also take RGB video-feed as input, a step closer to the complex sensory visual input that humans use,  and a step closer to agents in a 'real world' environment. Game environments provide well-defined domains for AI research, and recent publications focus on new, increasingly sophisticated game challenge domains. 

</p>
<p>In previous sections of the primer, We introduced many core reinforcement learning concepts with the grid environment. We can extend these to the Atari game Pong. 
</p>
<div align="center">
<video width="320" height="240" controls>
  <source src="images/pong_orig.mp4"  type="video/mp4">
</video>
<br><b><i>DQN-trained agent on right</b></i>
</div>
<p>
<header>
<h3> 2. Mapping grid concepts to Atari games </h3>
</header>
<ul><li> The <b>state</b> is the input perceived by the agent. Pong's state space is far greater than grid's. For simplicity, envision that a Pong state is a frame from the video input. Every possible permutation of pixels (which includes all possible pixel configurations for the positions of two paddles and the ball) make up the possible states.</li></ul> 
<ul><li> The <b>actions</b> are the legal actions available in Pong (move the agent's paddle up, down, no movement)</li></ul> 
<ul><li>The <b> reward signal</b> is +1 or -1, depending on whether the agent or the opponent scores. </ul>
<ul><li>The <b>policy</b> is the mapping of states to actions. Let's use a human player as an analogy. When a human sees a ball cresting towards her, she selects a series of actions to move the paddle in preparation for the shot and - based on an assessment of her opponent's position – angles the paddle to best score. The policy is the sequence of actions based on the changing states (which represent the positions of the player, opponent and ball). </li></ul>
<ul><li>The state's <b>value</b> represents how desirable or 'good' it is. A state where the opponent is about to score (e.g. the agent's paddle is at the top right corner, while as the ball is about to reach the bottom right corner) is an undesirable or low-value state. Conversely, a state where the agent's paddle is about to strike the ball is a high-value state. </li></ul>
</p>
Through Pong, we can identify some of the challenges related to reinforcement learning in more complex environments.
<ul><li><b>Large state-action spaces: </b>In the grid environments, it was possible to experience, compute and store $Q(s,a)$ values for each state-action pair. But, as states grow more complex, this is computationally infeasible. Consider Go, the Chinese board game, which has a board with $10^{170}$ possible states. Instead, we need to find ways to approximate value functions. With DQN, a neural network is used to predict $Q(s,a)$ values.</li></ul>
<ul><li><b>Uncertainty:</b> In the grid environment, the agent's action completely determines the next state $(S_{t+1})$. In Pong, the agent's action is only one of several lever that impact the next state– there is also the motion of the ball (depending on where and how it is introduced), and the opponent's actions. </li></ul>
<ul><li><b>Reward assignment:</b> The reward signal is only received when a point is scored – for a maximum of $+21$ per game. This is sparse when compared to the number of actions taken per game, and it is delayed feedback – the agent must learn to correctly assign a reward to the relevant sequence of states and actions.</li></ul>
<p>
</p>
<header>
<h3> 3. AI in the real world </h3>
</header>
We now have the conceptual understanding to consider two key real-world policy challenges – reward hacking and safe exploration. <p></p>
This section is a brief excerpt of examples drawn from  <a href="https://arxiv.org/pdf/1606.06565.pdf">‘Concrete Problems in AI safety’</a> a research paper by Google Brain and OpenAI, which provides a more comprehensive overview of problems and potential solutions. <p></p>

<ul><li><b>Reward Hacking</b></li></ul>
<i>“How can we ensure that the cleaning robot won’t game its
reward function? For example, if we reward the robot for achieving an environment free of
messes, it might disable its vision so that it won’t find any messes, or cover over messes with
materials it can’t see through, or simply hide when humans are around so they can’t tell it
about new types of messes.”</i><p></p>

As an agent looks to maximise its reward, it may identify policies that were not per the designer’s intent. In a video game, reward hacking can look like the OpenAI agent trained to play the video game Coastrunners. <p></p> From <a href = "https://openai.com/blog/faulty-reward-functions/">OpenAI</a> – “Despite repeatedly catching on fire, crashing into other boats, and going the wrong way on the track, our agent manages to achieve a higher score using this strategy than is possible by completing the course in the normal way. Our agent achieves a score on average 20 percent higher than that achieved by human players.”
<p></p>

<div align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/tlOIHko8ySg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<ul><li><b>Safe exploration</b></li></ul>
<i>“How do we ensure that the cleaning robot doesn’t make exploratory moves with very bad repercussions? For example, the robot should experiment with mopping strategies, but putting a wet mop in an electrical outlet is a very bad idea.” </i><p></p>

As an agent looks for the optimal policy in a new (or little-explored) environment, how can it safely explore an environment it doesn’t know very well? <p></p>

We discussed the $\epsilon$-greedy strategy – selecting a random action with $\epsilon$-probability during the training process– which does not factor in the potential for harm. One way to potentially mitigate this is through training an agent in a simulated environment before releasing it in the real-world.  <p></p>
Learn more: <a href="https://arxiv.org/pdf/1606.06565.pdf">‘Concrete Problems in AI safety’</a> 
<p></p>
</div>
</div>
<header>
<h3> 3. Next Steps</h3>
</header>
This primer was intended to provide a soft introduction to RL technical fundamentals, and some initial discussion that links them to RL research. For additional detail, consider reading the supplement on the Q-learning algorithm (if you know some Python, it includes an implementation of the DQN with the OpenAI environment). 

To delve more deeply into RL concepts, David Silver's <a href='https://www.youtube.com/watch?v=2pWv7GOvuf0'>RL lecture series </a> is available on YouTube and and Rich Sutton and Andrew Barto's seminal textbook 'Reinforcement Learning: An Introduction' is available <a href="http://incompleteideas.net/book/bookdraft2017nov5.pdf">free on-line</a> .

</div>
</body>
</html>
