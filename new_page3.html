---
layout: default
---

<html>
<style>
#content {
  float: left;
  margin: 0;
}
text {
  font: 11px "Proxima Nova",sans-serif;
  text-anchor: left;
}

.column {
  float: left;
  width: 45%;
  padding: 10px;
}
.row::after {
  content: "";
  clear: both;
  display: table;
}
td {
border-left: solid 0.5px black;
text-align: center;
font: 9px "Proxima Nova",sans-serif;
} 

th {
text-align: center;
font: 9px "Proxima Nova", sans-serif;
font-weight: bold;
} 

td:nth-last-child(2) {text-align:left;}
td:nth-last-child(1) {text-align:left;}


</style>
<head>
	<script src="https://d3js.org/d3.v4.min.js"></script>
	<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>
 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
</head>
<body>
 <header>
    <h1>A reinforcement learning primer</h1>
    <h2><a href="new_page2.html"><img src="images/left_arrow.png"style="width:8px;height:13px;"></a> 3/5 Value Functions <a href="new_page4.html"><img src="images/right_arrow.png"style="width:10px;height:15px;"></a></h2>
  </header>
<div id="content">
<div id="" style="overflow-y:scroll;">
<h3> i. Introducing Values </h3>

<p>A state's <b>reward</b> can be considered an indication of how 'good' it is immediately, while its <b>value</b> indicates how 'good' it will be in the long-term. </p>


<p>To provide more intuition on the distinction between a state's reward and its value, consider a revised grid environment (below) with labelled states and all +1 rewards revealed. Identify the optimal policy, with two additional constraints:</p>
<ul><li>The game must be completed within 4 moves.</li> </ul>
<ul><li>The game must begin by moving to State 2. This first move has been made for you – you have 3 left. </ul></li>
</p>
<div class = "row">
<div class="column">
<div id="grid2"></div>
<script src="grids/grid9.js" type="text/javascript"></script>
</div>
<div class="column">
<div id="grid1"></div>
<script src="grids/grid10.js" type="text/javascript"></script>
</div>
</div>
<p><u><i>Q: When in State 2: Do you move right to State 3 or down to State 5? </i></u> </p>
<p> Based on their rewards, State 3 and State 5 are equally desirable (they each return $+1$). But we know they are not equivalent – a policy that moves from State 2 to State 5 returns a higher total reward.  
<div class = "row">
<div class="column">
<div id="grid4"></div>
<script src="grids/grid11.js" type="text/javascript"></script>
</div>
<div class="column">
<div id="grid5"></div>
<script src="grids/grid12.js" type="text/javascript"></script>
</div>
</div>
<ul><li>Policy #1 (above) displays the optimal policy followed after moving from State 2 to State 3 –  it yields a cumulative game reward of $+1$. </li> </ul>
<ul> <li> Policy #2 (above) displays the optimal policy followed after moving from State 2 to State 5 – it yields a cumulative game reward of $+2$. </li> </ul>
Policy #2 is the optimal policy for the game. 
</p>
<header>
<h3> ii. Rewards, Values, and Q-values </h3>
</header>
<p> This primer focuses on <b>value-based methods</b>, a specific type of RL algorithm. An agent learns the <b>value</b> of states and corresponding actions, which then guides the agent's policies. 
</p>
There are different methods to calculate value, but we will use one called <b> Q-value</b>, represented by $Q(s,a)$, where $s$ represents the current state and $a$ represents the selected action. A Q-value can be considered the highest cumulative reward possible by selecting a given action in a given state.  
</p>
<ul><li>The highest value possible (beginning in State 2) by choosing to move to State 3 is $+2$ (following this policy: State 3- State 2- State 5). </li></ul>
<ul><li>The highest value possible (beginning in State 2) by choosing to move to State 5 is $+3$ (following this policy: State 5-State 10-15). </li></ul>
</p>
</p>
</p>
These are the Q-values!
</p> The Q-value can be decomposed into: the immediate reward of moving to the new state + the cumulative reward of pursuing the optimal policy from there-on-out.
</p>
This can be written mathematically as: 
$$Q(s,a) = r_{t} + max[(Q(s_{t+1},a_{t+1})]$$
</p>

<p>To be more explicit, let's work through the steps to calculate $Q(s,a)$ for State 2 to 3. </p>
$$Q(\text{State 2},\text{to State 3}) = 1 + max[1, 0, 0] $$
$$Q(\text{State 2},\text{to State 3})= 1 + 1$$
$$Q(\text{State 2},\text{to State 3}) =2$$

<ul><li>$r_{t}$ is the reward received in State 3: $+1$</ul></li>
<ul><li>$max[(Q(s_{t+1},a_{t+1})]$ is the maximum Q-value beginning in State 3. There are 3 Q-values for State 3 (one Q-value for each legal action), within the constraints of this particular game. 
<ul><li>$Q(s = 3, a = $ right$),$ The optimal policy from here is: State 2-5, which yields a cumulative reward of $0 + 1 = +1$. </li></ul>
<ul><li>$Q(s = 3, a = $ down$),$ The optimal policy from here is: State 7-10, each which yields a cumulative reward of $-1 + 1 = 0$. </li></ul> 
<ul><li>$Q(s = 3, a = $ right-down diagonal$),$ The optimal policy from here is either State 6-5 or 6-10, both which yield a cumulative reward of $-1+1 = 0$. </li></ul> </li></ul>
</li> </ul> 
</li></ul>




<ul><li>The maximum $Q(s,a)$ value for State 5 is $+3$.
<ul><li>There are 8 Q-values for State 5 (one Q-value for each legal action), within the constraints of this game. 
<ul><li>$Q(s = 5, a = $ right-down-diagonal$),$ The optimal policy is: State 5-10-15, which yields a cumulative reward of $+3$. </li></ul>
<ul><li>$Q(s = 5, a = $ down$),$ The optimal policy is: State 9-10-15, which yields a cumulative reward of $+2$. </li></ul> 
<ul><li>And so on... </li></ul> </li></ul>
</li> </ul> 
</li></ul>
</p>
<p>Let's consider a trained agent approaching the grid. The first move has been made, and the agent is in State 2. When assessing whether to move to State 3 (max. Q-value: $+2$) or State 5 (max. Q-value: $+3$), the agent will select State 5 – the state with the highest possible Q-value.</p>
<p><i>This section does not focus on training (how the agent learns Q-values), but click <a href="new_page1.html">here</a> for a supplementary page that details a learning algorithm known as Q-learning.</i> </p>
<header>
<h3> iii. Discounting </h3>
</header>
<p> 
Consider two policies: Policy A yields +100 over 1 time step, while Policy B yields +100 in cumulative reward over 100 time steps. Are both policies equivalent, or would you prefer Policy 2? 
</p>
<p>
We can express this preference mathematically by introducing two additional terms: <b>time step ($t$)</b> and <b>discount factor ($\gamma$)</b> and, for ease, some mathematical notation. 

<header>
<h4>a. Time Steps</h4>
</header>
<p> There is no external notion of time in this game – there is no clock runnning down in the background. But we can consider each action taken to represent some time period or <b>time-step</b>. All states, actions, and rewards are represented by the time-step in which they are experienced (states  \(s\)), taken (actions \(a\)), or received (rewards \(r\)). If we were to introduce clock-time, we could consider each minute a time-step or, alternatively, each second a time-step (depending on the game) </p>

<p> To apply this new language, let's consider the following sequence:
<ul><li> The game begins at time-step 0, and I move from Square 0 to Square 1 and receive reward $-1$.</li> </ul> 
<ul><li> In time-step 1, I move from Square 1 to Square 5 and receive reward $+1$. </li></ul> </p>

<header>
<h4>b. Discount Factor</h4>
</header>
<p>We can weight rewards received in the future, so that they are less valuable than rewards received immediately. If time step $t$ begins at 0, the reward is discounted at discount factor ($\gamma^t$). At $t_0$, the reward $r_0$ is discounted at $\gamma^0$; at $t_1$, the reward $r_1$ is discounted at $\gamma^1$ and so on. If $\gamma = 0$, future rewards are discounted completely – only immediate rewards matter. If $\gamma = 1$, future rewards matter the same as those received immediately (as in all the previous examples).</p>

<p>Let's return to the question posed at the beginning of Section iii:</p>
<ul><li>If the discount factor ($\gamma$) = 1, then future rewards are weighted the same as immediate rewards. <ul><li>Policy A \(=\) Policy B  \(= 100\).</li></ul></li></ul>
<ul><li>If $\gamma$ = 0.8, then Policy A has a cumulative discounted reward of $100$, while Policy B has one of $5$ (and is much less desirable). 
<ul><li><i>Calculations:</i> Policy A $ = 100*0.8^0$; Policy B =  $1*0.8^0+ 1*0.8^1 + 1*0.8^2...1*0.80^{99} \approx 5$ </li></ul></li> </ul>
<ul><li>If $\gamma$ = 0.5, then Policy A has a cumulative discounted reward of $100$, while Policy B has one of $2$ (and is even less desirable).<ul><li> <i>Calculations:</i> Policy A $ = 100*0.5^0$; Policy B =  $1*0.5^0+ 1*0.5^1 + 1*0.5^2...1*0.50^{99} = 2$</li></ul></li></ul>
</p>
<p>To gain more fluency with discounted rewards, interact with the grid below. Move the slider to adjust the discount factor, and click on any square to construct different policies and evaluate the total discounted reward.
<ul><li> Note: The total discounted reward is not always the same as the state-action's Q-value. This because the Q-value is based on a (known or estimated) <b>optimal</b> policy, not just any policy. </li></ul>
</p>
<div class = "row">
<div class="column">
<div id="grid3"></div>
<p>
  <label for="nRadius" 
         style="display: inline-block; width: 180px; text-align: left">
         Discount Factor ($\gamma$)= <span id="nRadius-value">0.5</span>       
  </label>
  <input type="range" min="0" max="1" step="0.1" id="nRadius">
</p>
<div class = "text_label">
</div>
</div>
<div class="column">
<table id="table"></table>
</div>
<script src="grids/grid14.js" type="text/javascript"></script>
</div>
</div>
</div>
</div>
</body>
</html>
